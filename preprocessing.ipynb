{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.85\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hỗ trợ phí vận chuyển combo 2 thuốc chữa cảm c...</td>\n",
       "      <td>có sản phẩm</td>\n",
       "      <td>thuốc chữa cảm cúm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fenty beauty - bắt sáng dạng lỏng liquid diamo...</td>\n",
       "      <td>có sản phẩm</td>\n",
       "      <td>bắt sáng</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tinh dầu massage baby oil sesam street nhập kh...</td>\n",
       "      <td>có sản phẩm</td>\n",
       "      <td>tinh dầu massage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flycam</td>\n",
       "      <td>có tên gọi</td>\n",
       "      <td>jjrc x11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikini, đồ bơi nữ, bộ bikini phong cách hàn cạ...</td>\n",
       "      <td>có họa tiết</td>\n",
       "      <td>họa tiết hình học</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject    predicate  \\\n",
       "0  hỗ trợ phí vận chuyển combo 2 thuốc chữa cảm c...  có sản phẩm   \n",
       "1  fenty beauty - bắt sáng dạng lỏng liquid diamo...  có sản phẩm   \n",
       "2  tinh dầu massage baby oil sesam street nhập kh...  có sản phẩm   \n",
       "3                                             flycam   có tên gọi   \n",
       "4  bikini, đồ bơi nữ, bộ bikini phong cách hàn cạ...  có họa tiết   \n",
       "\n",
       "               object label  \n",
       "0  thuốc chữa cảm cúm     1  \n",
       "1            bắt sáng     1  \n",
       "2    tinh dầu massage     1  \n",
       "3            jjrc x11     1  \n",
       "4   họa tiết hình học     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"./data/vn_dataset_version/vn_all_triple.csv\"\n",
    "dataset = pd.read_csv(data_path, dtype=object)\n",
    "dataset.columns = ['subject', 'predicate', 'object']\n",
    "dataset['label'] = ['1']*len(dataset)\n",
    "dataset.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting to train, test, valid set\n",
    "###### Using train_test_split_no_unseen funciton, that ensures the entities in test and val set are \"seen\" in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = len(dataset)\n",
    "test_train_set, valid_set = train_test_split_no_unseen(dataset.to_numpy(dtype = str), test_size = int(VAL_SIZE * DATASET_SIZE), seed=config.SEED, allow_duplication=True)\n",
    "train_set, test_set = train_test_split_no_unseen(test_train_set, test_size = int(TEST_SIZE * DATASET_SIZE), seed=config.SEED, allow_duplication=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173021"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save to the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df = pd.DataFrame(train_set)\n",
    "raw_train_df.columns = ['subject', 'predicate', 'object', 'label']\n",
    "\n",
    "raw_test_df = pd.DataFrame(test_set)\n",
    "raw_test_df.columns = ['subject', 'predicate', 'object', 'label']\n",
    "\n",
    "raw_val_df = pd.DataFrame(valid_set)\n",
    "raw_val_df.columns = ['subject', 'predicate', 'object', 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_file_path = \"./data/vn_dataset_version/entity_and_title.txt\"\n",
    "raw_train_df.to_csv(raw_train_file_path, index=False)\n",
    "\n",
    "raw_test_file_path = \"./data/vn_dataset_version/raw_test.csv\"\n",
    "raw_test_df.to_csv(raw_test_file_path, index=False)\n",
    "\n",
    "raw_val_file_path = \"./data/vn_dataset_version/raw_val.csv\"\n",
    "raw_val_df.to_csv(raw_val_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CORRUPTION_RATIO = 5\n",
    "TEST_CORRUPTION_RATIO = 1\n",
    "VAL_CORRUPTION_RATIO = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all triple of dataset \n",
    "npdataset = dataset.to_numpy(dtype = str)\n",
    "all_triple = set([(h,r,t) for h,r,t,l in npdataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all entites\n",
    "with open(\"./data/vn_dataset_version/entity_and_title.txt\", \"r\", encoding='utf8') as f:\n",
    "    entities = set()\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        entities.add(line)\n",
    "    if '' in entities:\n",
    "        entities.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corruptions function\n",
    "# data_arr : [[h,r,t,l]*len]\n",
    "# entites : set()\n",
    "# all_triple : set(tuple(h,r,t))\n",
    "def create_corruption(data_arr, entities, all_triple, ratio):\n",
    "    corrupt_arr = []\n",
    "    for head, rel, tail, label in data_arr:\n",
    "        # print(head, rel, tail)\n",
    "        corr_label = 0\n",
    "        for i in range(ratio):\n",
    "            rnd = random.random()\n",
    "            if rnd <= 0.5:\n",
    "                # corrupting head\n",
    "                tmp_head = ''\n",
    "                while True:\n",
    "                    tmp_ent_list = entities.copy()\n",
    "                    tmp_ent_list.remove(head)\n",
    "                    tmp_ent_list = list(tmp_ent_list)\n",
    "                    tmp_head = random.choice(tmp_ent_list)\n",
    "                    tmp_triple = (tmp_head, rel, tail)\n",
    "                    if tmp_triple not in all_triple:\n",
    "                        break                    \n",
    "                corrupt_arr.append([tmp_head, rel, tail, corr_label])\n",
    "            # end if\n",
    "            else:\n",
    "                # corrupting tail\n",
    "                tmp_tail = ''\n",
    "                while True:\n",
    "                    tmp_ent_list = entities.copy()\n",
    "                    tmp_ent_list.remove(tail)\n",
    "                    tmp_ent_list = list(tmp_ent_list)\n",
    "                    tmp_tail = random.choice(tmp_ent_list)\n",
    "                    tmp_triple = (head, rel, tmp_tail)\n",
    "                    if tmp_triple not in all_triple:\n",
    "                        break\n",
    "                corrupt_arr.append([head, rel, tmp_tail, corr_label])\n",
    "            # end else \n",
    "        # end for\n",
    "    #end for\n",
    "    return np.array(corrupt_arr)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating corruptions for each \"set\" with different true triple/corruption ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corrupt_arr = create_corruption(train_set, entities, all_triple, TRAIN_CORRUPTION_RATIO)\n",
    "train_with_corr = np.concatenate((train_corrupt_arr, train_set))\n",
    "train_with_corr = train_with_corr[train_with_corr[:, 0].argsort()]\n",
    "\n",
    "test_corrupt_arr = create_corruption(test_set, entities, all_triple, TEST_CORRUPTION_RATIO)\n",
    "test_with_corr = np.concatenate((test_corrupt_arr, test_set))\n",
    "test_with_corr = test_with_corr[test_with_corr[:, 0].argsort()]\n",
    "\n",
    "val_corrupt_arr = create_corruption(valid_set, entities, all_triple, VAL_CORRUPTION_RATIO)\n",
    "val_with_corr = np.concatenate((val_corrupt_arr, valid_set))\n",
    "val_with_corr = val_with_corr[val_with_corr[:, 0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_corr_file_path = \"./data/vn_dataset_version/train_with_corr.csv\"\n",
    "train_with_corr_df = pd.DataFrame(train_with_corr, columns = ['subject', 'predicate', 'object', 'label'])\n",
    "train_with_corr_df.to_csv(train_with_corr_file_path, index=False)\n",
    "\n",
    "test_with_corr_file_path = \"./data/vn_dataset_version/test_with_corr.csv\"\n",
    "test_with_corr_df = pd.DataFrame(test_with_corr, columns = ['subject', 'predicate', 'object', 'label'])\n",
    "test_with_corr_df.to_csv(test_with_corr_file_path, index=False)\n",
    "\n",
    "val_with_corr_file_path = \"./data/vn_dataset_version/val_with_corr.csv\"\n",
    "val_with_corr_df = pd.DataFrame(val_with_corr, columns = ['subject', 'predicate', 'object', 'label'])\n",
    "val_with_corr_df.to_csv(val_with_corr_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10429ee993181f60e6e2fb8d8898bdd335e1dca66225fda34ad9c93ac779e1d1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
